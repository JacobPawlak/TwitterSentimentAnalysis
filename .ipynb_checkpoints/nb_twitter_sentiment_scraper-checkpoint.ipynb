{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's start by installing all of our libraries and dependancies\n",
    "import sys\n",
    "!{sys.executable} -m pip install emoji\n",
    "!{sys.executable} -m pip install TwitterAPI\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install bs4\n",
    "!{sys.executable} -m pip install selenium\n",
    "!{sys.executable} -m pip install nltk\n",
    "!{sys.executable} -m pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if this is your first time using the nltk library, you will need to download some stuff\n",
    "#you should just use the download all tool, it wont take very long and it's nice to have the whole library\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i am going to be using the twitter api to pull down full archive queries\n",
    "from TwitterAPI import TwitterAPI\n",
    "from TwitterAPI import TwitterPager\n",
    "#bringing this in for the sleep() method\n",
    "import time\n",
    "#json lib for json.dump()\n",
    "import json\n",
    "#going to convert my dicts to dataframes for csv output\n",
    "import pandas\n",
    "#bringing in the natural language tool kit - one of our favorites, this is a huge environment requirement\n",
    "#VADER is the sentiment analizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#going to use the tokenize method to chunk up our tweet data\n",
    "from nltk import tokenize\n",
    "#since we cant do sentiment over emojis with vader (i dont think?) i need this to remove emojis\n",
    "import emoji\n",
    "#regex library for cleaning strings and matching tweets\n",
    "import re\n",
    "#using these two libs for the datafile i am going to pass in (simulating some env variables)\n",
    "import os\n",
    "#i need a few libs for webscraping twitter since it is a dynamic site that has some weird modal stuff idk\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#importing tweepy so i can grab the tweets scraped from the webscraper\n",
    "import tweepy\n",
    "#importing the streamlit library for some fun graphing and dataviz\n",
    "#import streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file_path):\n",
    "    #since i will probably reuse this helper i am going to make sure it can accept multiple file types\n",
    "    #for pulling in csv into pandas dataframe\n",
    "    if(file_path.split('.')[-1].lower() == 'csv'):\n",
    "        dataframe = pandas.read_csv(file_path)\n",
    "        print('returning a pandas dataframe')\n",
    "        return dataframe\n",
    "    #for pulling json files into [{}] structures\n",
    "    elif(file_path.split('.')[-1].lower() == 'json'):\n",
    "        data_file = open(file_path, 'r')\n",
    "        data_list = []\n",
    "        for line in data_file:\n",
    "            data_list = eval(line)\n",
    "        print('returning a list of dictionaries')\n",
    "        return data_list[0]\n",
    "    #basically same thing as the json file, just saved as a text file\n",
    "    elif(file_path.split('.')[-1].lower() == 'txt'):\n",
    "        data_file = open(file_path, 'r')\n",
    "        data_list = []\n",
    "        for line in data_file:\n",
    "            data_list = eval(line)\n",
    "        print('returning a list of dictionaries')\n",
    "        return data_list[0]\n",
    "    #if the filetype doesnt match on of our cases, just return\n",
    "    else:\n",
    "        print(\"You passed in an unrecognized file type, please use a csv, json, or txt file\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_api(c_key, c_secret, a_token, a_secret):\n",
    "    #using an OAuth api connection to twitter with my personal twitter account.\n",
    "    # i will be passing in these keys and tokens from a datafile passed in via command line\n",
    "    api = TwitterAPI(c_key, c_secret, a_token, a_secret)\n",
    "\n",
    "    #return the api object to the main\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_senti_analysis(tweet_object):\n",
    "#bringing in the cleaned tweet so i can add the vader sentiment scores to them\n",
    "    senti_tweet = tweet_object\n",
    "    senti_tweet['senti_score_pos'] = 0\n",
    "    senti_tweet['senti_score_neu'] = 0\n",
    "    senti_tweet['senti_score_neg'] = 0\n",
    "    senti_tweet['senti_score_com'] = 0\n",
    "\n",
    "    tweet_text = senti_tweet['text']\n",
    "    #now i want to take out all of the emoji or emoji like things in the text since VADER (again, i dont think) cannot run\n",
    "    # over emojis\n",
    "    tweet_text = emoji.demojize(tweet_text)\n",
    "    #i also dont want hashtags in there so I am going to pull that out too with the regex match\n",
    "    # this regex looks for the hashtag symbol followed by a word, then replaces it with an empty string\n",
    "    tweet_text = re.sub(r'#\\w*', \"\", tweet_text)\n",
    "    #this next one is to pull out usernames, same idea with regex match\n",
    "    tweet_text = re.sub(r'@\\w*', \"\", tweet_text)\n",
    "    #now i need to remove all of the urls that get left in the tweet (either a gif link or the extended tweet link)\n",
    "    tweet_text = re.sub(r'(\\w*\\.)?\\w*\\.\\w*', \"\", tweet_text)\n",
    "    #the line above should get any www.xyz.com formats, the one below should get any http(s)://(www.)blah.com\n",
    "    tweet_text = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', \"\", tweet_text)\n",
    "    #now that the tweet text is pretty cleaned, I need to use the nltk tokenizer to split up the text (will result in a list of strings)\n",
    "    tweet_sents = tokenize.sent_tokenize(tweet_text)\n",
    "    #sometimes the tweet was composed of only things that got cleaned out, so i need to set a limit/count of sentences\n",
    "    num_sents = len(tweet_sents)\n",
    "    #setting up the scores that will be edited when I bring in VADER\n",
    "    score_pos = 0\n",
    "    score_neu = 0\n",
    "    score_neg = 0\n",
    "    score_com = 0\n",
    "    #here I will correct the case that num_sents is 0 (all emojis or something, no text)\n",
    "    if(num_sents < 1):\n",
    "        #gotta do this to avoid dividing by zero (from experience)\n",
    "        num_sents = 1\n",
    "    #time to bring in the big guy. the darth. the master.\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    #now I will run vader over each sentence. vader is really nice because it picks up on so much more than just POS and token scores, it takes the whole sentence into account\n",
    "    # and produces a score that incorporates syntax and expression\n",
    "    for sent in tweet_sents:\n",
    "        #calling vader over the string will return a dictionary of scores\n",
    "        scores = vader.polarity_scores(sent)\n",
    "        score_pos += scores['pos']\n",
    "        score_neu += scores['neu']\n",
    "        score_neg += scores['neg']\n",
    "        score_com += scores['compound']\n",
    "\n",
    "    #okay it's time to add the scores back into the tweet dictionary.\n",
    "    senti_tweet['senti_score_pos'] = score_pos/num_sents\n",
    "    senti_tweet['senti_score_neu'] = score_neu/num_sents\n",
    "    senti_tweet['senti_score_neg'] = score_neg/num_sents\n",
    "    senti_tweet['senti_score_com'] = score_com/num_sents\n",
    "\n",
    "    #returns a dict object (the tweet dict with senti scores)\n",
    "    return senti_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet_object):\n",
    "#staring a new dictionary to add all of the cleaned data to\n",
    "    cleaned_tweet = {}\n",
    "\n",
    "    #here i will be pulling out some of the data from the raw tweet object\n",
    "    cleaned_tweet['id_str'] = tweet_object['id_str']\n",
    "    #grab the screen name (handle) of the tweet owner\n",
    "    cleaned_tweet['screen_name'] = tweet_object['user']['screen_name']\n",
    "    #also grab the user name (different from the handle)\n",
    "    cleaned_tweet['user_name'] = tweet_object['user']['name']\n",
    "\n",
    "    #pull out the date and time from the timestamp of when the tweet was created, we are going to split up the date and time\n",
    "    #here is an example of the \"created_at\": \"Tue Oct 08 18:42:30 +0000 2019\"\n",
    "    created_date = tweet_object['created_at']\n",
    "    #splitting by whitespace to get a list, and since i know all of the indecies i can build the date and time strings\n",
    "    created_date = created_date.split()\n",
    "    #you can follow along with the example above \n",
    "    year = created_date[-1]\n",
    "    #made a small list of month abrev. to index for a quick str month -> int month\n",
    "    months = ['Jan', 'Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "    month = created_date[1]\n",
    "    #0 index start\n",
    "    month_digit = 1 + months.index(month)\n",
    "    day = created_date[2]\n",
    "    time = created_date[3]\n",
    "    new_date = str(str(year) + '-' + str(month_digit) + '-' + str(day))\n",
    "    cleaned_tweet['date'] = new_date\n",
    "    cleaned_tweet['time'] = str(time)\n",
    "\n",
    "    #for some steps later it will be important to know if it a top level tweet from the brand or from a comment\n",
    "    if(tweet_object['in_reply_to_status_id_str'] == None):\n",
    "        cleaned_tweet['in_reply_to_status_id_str'] = \"Brand Tweet\"\n",
    "    else:\n",
    "        cleaned_tweet['in_reply_to_status_id_str'] = tweet_object['in_reply_to_status_id_str']\n",
    "\n",
    "    #so now that twitter allows 280 chars in a tweet, we have to account for the 'full_text' flag\n",
    "    if('extended_tweet' in tweet_object.keys()):\n",
    "        cleaned_tweet['text'] = tweet_object['extended_tweet']['full_text']\n",
    "    else:\n",
    "        cleaned_tweet['text'] = tweet_object['text']\n",
    "\n",
    "    #the tweets also have a retweet and favorite count that i can pull\n",
    "    cleaned_tweet['retweet_count'] = tweet_object['retweet_count']\n",
    "    cleaned_tweet['favorite_count'] = tweet_object['favorite_count']\n",
    "\n",
    "    #I can also build the url (like you would see if you browsed on a desktop)\n",
    "    cleaned_tweet['url'] = \"https://twitter.com/{}/status/{}\".format(cleaned_tweet['screen_name'], cleaned_tweet['id_str'])\n",
    "\n",
    "    #returns a dict object (the tweet)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(datafile_, brand_tweets_list):\n",
    "#bringing in the datafile and the brand list\n",
    "    datafile = datafile_\n",
    "    brand_tweets = brand_tweets_list\n",
    "\n",
    "    #going to make another save file too while im here in the preamble\n",
    "    savefile = open('comments_for_' + str(datafile['output_file_name']) + '.txt', 'w')\n",
    "\n",
    "    #here is where i am setting up the tweepy connection. I am using tweepy because it is an easy and free connection\n",
    "    #  to twitter (outside of the official twitterapi library) and it has this cool get_status() method ill use later\n",
    "    auth = tweepy.OAuthHandler(str(datafile['c_key']), str(datafile['c_secret']))\n",
    "    auth.set_access_token(str(datafile['a_token']), str(datafile['a_secret']))\n",
    "    api = tweepy.API(auth)\n",
    "\n",
    "    #gotta bring in our reliable CHROMEDRIVER baby\n",
    "    driver = webdriver.Chrome('./Chrome/' + str(datafile['chrome_version']) + '/chromedriver')\n",
    "\n",
    "    #setting up an empty list to put comment tweet obj in there\n",
    "    comment_tweets = []\n",
    "\n",
    "    #quick little counter for the tweets\n",
    "    tweet_count = 0\n",
    "\n",
    "    #it's time to start pulling tweets from the twitter website, i can loop over the brand tweets list and point the webscraper at the url\n",
    "    for brand_tweet in brand_tweets:\n",
    "        \n",
    "        tweet_count += 1\n",
    "        print(\"Scraping brand tweet #{}: {}\".format(tweet_count, brand_tweet['text']))\n",
    "\n",
    "        #going to call the chrome browser\n",
    "        driver.get(brand_tweet['url'])\n",
    "        time.sleep(4)\n",
    "\n",
    "        '''\n",
    "        here is where i need to add the part where it hits page down a whole bunch because if i dont do it here or manually it will only pull like 5 comments per tweet\n",
    "        '''\n",
    "\n",
    "        #this is the combination of selenium and bs4 that i have been able to get to work for twitter\n",
    "        soup = BeautifulSoup(driver.page_source, 'html5lib')\n",
    "        #after experience i know that i can grab all of the links (each tweet in the thread is like a link to the individual page, and i can pull the id out of the link)\n",
    "        comment_links = soup.find_all('a', attrs={'href': re.compile(r'/*/status/[0-9]+$')})\n",
    "        comment_tweet_ids = [comment['href'].split('/')[-1] for comment in comment_links]\n",
    "\n",
    "        for comment_tweet_id in comment_tweet_ids:\n",
    "\n",
    "            #here is where i start to bring in the tweepy method for getting a status\n",
    "            try:\n",
    "                new_comment = add_senti_analysis(clean_tweet(api.get_status(comment_tweet_id)._json))\n",
    "                if(new_comment['in_reply_to_status_id_str'] == \"Brand Tweet\"):\n",
    "                    new_comment['in_reply_to_status_id_str'] = brand_tweet['id_str']\n",
    "                print(new_comment['text'])\n",
    "                #add it to the list of comment tweets\n",
    "                comment_tweets.append(new_comment)\n",
    "                savefile.write(str(new_comment) + '\\n')\n",
    "                \n",
    "                #reset our rate_limit_hit error counter\n",
    "                rate_limit_errs = 0\n",
    "\n",
    "            #since this is a try clase we need to look for exceptions. the ones i know of are the tweepy rate limit error or a chromedriver error (cant do a whole lot about)\n",
    "            except tweepy.RateLimitError as err:\n",
    "                print(\"Oh no, we seem to have run into the rate limiter from tweepy: {}. Sleeping for 3 minutes to hold\".format(err))\n",
    "                time.sleep(180)\n",
    "            \n",
    "            #a more general exception\n",
    "            except Exception as err:\n",
    "                print(\"Oh no, we seem to have run into a strange error: {}. Please check to see if this will impare performance. Sleeping for 10 seconds to hold\".format(err))\n",
    "                time.sleep(10)\n",
    "                rate_limit_errs += 1\n",
    "                \n",
    "                #if there are like more than 5 errors that werent rate limit errors then we need to break\n",
    "                if(rate_limit_errs > 4):\n",
    "                    driver.quit()\n",
    "                    #time to dump our data out to the directory\n",
    "                    with open('comments_for_' + str(datafile['output_file_name']) + '.json', 'w') as comments_json:\n",
    "                        json.dump(comment_tweets, comments_json)\n",
    "                    comment_tweets_df = pandas.DataFrame(comment_tweets)\n",
    "                    comment_tweets_df.to_csv('comments_for_' + str(datafile['output_file_name']) + '.csv', encoding='utf-8')\n",
    "                    break\n",
    "\n",
    "                continue\n",
    "\n",
    "        #i know that the selenium browser will get slow after opening so many pages (it is chrome after all)\n",
    "        # so every 25 brand tweets we are going to close the browser and reopen it\n",
    "        if(tweet_count % 25 == 0):\n",
    "            driver.quit()\n",
    "            time.sleep(2)\n",
    "            driver = webdriver.Chrome('./Chrome/' + str(datafile['chrome_version']) + '/chromedriver')\n",
    "\n",
    "    #closing the save file to free that mem back up        \n",
    "    savefile.close()\n",
    "    \n",
    "    #after it's done pulling comments per tweet, lets report the successes and return the comments list\n",
    "    try:\n",
    "        print(\"Total number of comments pulled: {} with an average of {} tweets per comment\".format(len(comment_tweets), (len(comment_tweets)/len(brand_tweets))))\n",
    "    except ZeroDivisionError as err:\n",
    "        print(\"There seems to be no tweets to scrape comments from! See log above for more details.\")\n",
    "        driver.quit()\n",
    "        \n",
    "    #returning a list of dictionaries (tweets)\n",
    "    return comment_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_brand_and_comments_on_id(datafile_, brand_tweets_df, comment_tweets_df):\n",
    "    #pulling in the params to local variables \n",
    "    datafile = datafile_\n",
    "    brand_df = brand_tweets_df\n",
    "    comment_df = comment_tweets_df\n",
    "\n",
    "    #easy little pandas merge, or a left inner join for you sql fans\n",
    "    merged_brand_comment_df = pandas.merge(brand_df, comment_df, left_on='id_str', right_on='in_reply_to_status_id_str', how='inner', suffixes=('_brand', '_comment'))\n",
    "    #writing it out to a file so we can directly upload it to a Power BI dashboard\n",
    "    merged_brand_comment_df.to_csv(\"merged_{}.csv\".format(str(datafile[\"output_file_name\"])), encoding=\"utf-8\")\n",
    "\n",
    "    #returning the merged dataframe\n",
    "    return merged_brand_comment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_raspi_datafile(output_file_name_, merged_brand_comment_df_):\n",
    "    #this little helper here is going to spit out a text file and csv file with the sentiment scores in a small format that can be uploaded to the raspi\n",
    "    #we will run this pretty much last in the script since we want to use the merged brand and comment table\n",
    "\n",
    "    #here we are bringing in the arguments to our local scope\n",
    "    output_file_name = output_file_name_\n",
    "    merged_df = merged_brand_comment_df_\n",
    "\n",
    "    #making a new dataframe for the raspi output, selecting the id's for the brand and comment tweets, and the respective senti scores\n",
    "    raspi_df = merged_df[['id_str_brand', 'senti_score_pos_brand', 'senti_score_neu_brand', 'senti_score_neg_brand', 'senti_score_com_brand', 'id_str_comment', 'senti_score_pos_comment', 'senti_score_neu_comment', 'senti_score_neg_comment', 'senti_score_com_comment']]\n",
    "    print(\"Creating files for raspberry pi extension...\")\n",
    "    raspi_df.to_csv(\"raspi_{}.csv\".format(str(output_file_name)), encoding=\"utf-8\")\n",
    "    raspi_df.to_csv(\"raspi_{}.txt\".format(str(output_file_name)), header=None, index=None, sep=' ', mode='a')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time to grab some of the variables that would have been passed in as system args\n",
    "datafile_path = str(input(\"Please enter the path to the datafile: \"))\n",
    "verbose = str(input(\"Verbose output? (Yes/no): \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### MAIN () ####################\n",
    "def main():\n",
    "\n",
    "    try:\n",
    "        datafile = open_file(datafile_path)\n",
    "    except Exception as err:\n",
    "        print(\"Oh no, there was an error ({}) trying to open the datafile {}, perhaps it was formatted incorrectly? Please try again\".format(err, datafile_path))\n",
    "        return\n",
    "    \n",
    "    #these two lists are going to hold all of the tweets as they get cleaned and sorted into Brand tweets or response tweets\n",
    "    TWEET_LIST = []\n",
    "    BRAND_LIST = []\n",
    "\n",
    "    #now i am going to setup the api connection with the helper function i wrote in the HELPERS section\n",
    "    #since this is not a garunteed connection, i am going to wrap it in a try/except clause\n",
    "    try:\n",
    "        api = connect_api(str(datafile['c_key']), str(datafile['c_secret']), str(datafile['a_token']), str(datafile['a_secret']))\n",
    "    except Exception as err:\n",
    "        print(\"Oh no, there was an error ({}) trying to connect to the Twitter API, perhaps the consumer/access keys are incorrect? Please try again.\".format(err))\n",
    "        return\n",
    "\n",
    "    #I am going to make a file to save the data in incase there is an error and the program crashes, i wont have lost everything (cant get requests back)\n",
    "    savefile = open(str(datafile['output_file_name']) + '.txt', 'w')\n",
    "\n",
    "    #so before i make the api request there are a few things to check, one of those being the max_results. if it is less than 10 the request will fail\n",
    "    if(int(datafile['max_results']) < 10):\n",
    "        datafile['max_results'] = \"10\"\n",
    "    #also going to check if the number is over 100 since I can only pull 100 tweets per request, and it might err out if i try more. tbh i think it sets it to max anyways, but might as well do it myself\n",
    "    elif(int(datafile['max_results']) > 100):\n",
    "        datafile['max_results'] = \"100\"\n",
    "\n",
    "    #heeeeeere it goes! time to make the api call! I will continue to make calls in a loop underneath this call.\n",
    "    request = api.request(\"tweets/search/fullarchive/:{}\".format(str(datafile['dev_environment'])), {\"query\": \"from:{} lang:en\".format(str(datafile['target_user_id'])), \"toDate\": str(datafile['toDate']), \"fromDate\": str(datafile['fromDate']), \"maxResults\": int(datafile['max_results'])})\n",
    "\n",
    "    #in the case that the number of tweets queried is greater than 100, we will require a 'next token' to grab the rest (or next 100) tweets from the api\n",
    "    # we are going to initiate it here\n",
    "    next_token = ''\n",
    "    #also going to set up a counter for the rate limiter (doing this after experience where it sat for ~3 hours on rate limit. it would have gone for a few weeks before the limit was refreshed)\n",
    "    rate_limit_hit = 0\n",
    "    #now it is time to loop!\n",
    "    while(True):\n",
    "\n",
    "        #time to check if the request went through and returned our tweets or returned an error\n",
    "        #from the docs i know that the request comes back with a status code \n",
    "        if(request.status_code is not 200):\n",
    "            #well if we got here we know there was an error, so lets let someone know about it\n",
    "            print(\"Oh no, there was an error recieved from the request ({})...\".format(request.status_code))\n",
    "            #okay so the only errors we need to address are the rate limit error, bad requests (hopefully only once), or internal server errors\n",
    "            #here is the bad request. all i need to do is let the user know that the request failed (there is no use trying again is the request is bad.)\n",
    "            if(request.status_code is 400):\n",
    "                print(\"Oh no, the error was a 400 - bad request. Please reformat your datafile and try again.\")\n",
    "                return\n",
    "            #okay so now if we make it here and it is a rate limit error code, i can just let the program run for a couple loops \n",
    "            if(request.status_code is 429):\n",
    "                #now to increment the rate limit count\n",
    "                rate_limit_hit += 1\n",
    "                #just so the program doesnt run forever, lets set a sentinal value\n",
    "                if(rate_limit_hit > 5):\n",
    "                    print(\"Oh no, it seems like the request limit has been hit, it's time to stop the program. Please check the https://developer.twitter.com/en/dashboard to make sure.\")\n",
    "                    #so instead of returning out of the program i am going to break so that I can save the tweets already collected\n",
    "                    break\n",
    "                #if the sentinal wasnt reached yet then the program can just sit for a few mins\n",
    "                print(\"Sleeping for 3 minutes to wait out the rate limit\")\n",
    "                time.sleep(180)\n",
    "                continue\n",
    "            #so these 50X error codes that we dont have really any control over so its best to just hang out like with the rate limit \n",
    "            if( (request.status_code is 500) or (request.status_code is 503) or (request.status_code is 504) ):\n",
    "                rate_limit_hit += 1\n",
    "                if(rate_limit_hit > 5):\n",
    "                    print(\"Oh no, there seems to be an internal server error from Twitter, time to break\")\n",
    "                    break\n",
    "                print(\"Sleeping for 3 minutes to wait out the rate limit\")\n",
    "                time.sleep(180)\n",
    "                continue\n",
    "            else:\n",
    "                #there must be some error that we dont know how to deal with just yet so its time to break\n",
    "                break\n",
    "        #wowzers so the request made it here, which means that we didnt hit the error code list and the request came through\n",
    "        #lets reset the rate limit count\n",
    "        else:\n",
    "            rate_limit_hit = 0\n",
    "\n",
    "        #time to extract the tweets from the request package.\n",
    "        for tweet in request:\n",
    "            if('text' in tweet):\n",
    "                #so this is kinda fun, we get to call the clean_tweet function inside the add_senti_analysis function\n",
    "                clean_senti_tweet = add_senti_analysis(clean_tweet(tweet))\n",
    "                \n",
    "                #now that the tweet is cleaned and has the senti scores added, it's time to add it to the list\n",
    "                TWEET_LIST.append(clean_senti_tweet)\n",
    "\n",
    "                #this is a section where we determine if the tweet was a top level tweet from the brand or a response to someone, etc. \n",
    "                if(clean_senti_tweet['in_reply_to_status_id_str'] == \"Brand Tweet\"):\n",
    "                    #if the tweet has the @ symbol in the first few chars then it's either a retweet or a response (most likely, it could be the case that it is a response with preamble...)\n",
    "                    if('@' not in clean_senti_tweet['text'][0:4]):\n",
    "                        BRAND_LIST.append(clean_senti_tweet)\n",
    "                \n",
    "                #just for safekeeping its time to write out to the save file\n",
    "                savefile.write(str(clean_senti_tweet) + '\\n')\n",
    "                #just so we arent left with a blank screen or just errors\n",
    "                print(clean_senti_tweet['text'])\n",
    "        \n",
    "        #this is the method we are going to use to check for that next token we set earlier\n",
    "        #this is to extract the _json payload that comes with the request\n",
    "        request_json = request.json()\n",
    "        if('next' not in request_json):\n",
    "            #if we are inside this if statement then there isnt a next token in the request, so in theory we have reached the end of the selection of tweets\n",
    "            break\n",
    "        #if we made it past that break statement than we know that the next token is there so lets grab it\n",
    "        next_token = request_json['next']\n",
    "        #okay now that we made it here its time to complete the loop and call the Twitter api again. all that is required is to add the next token in the params dictionary like so:\n",
    "        request = api.request(\"tweets/search/fullarchive/:{}\".format(str(datafile['dev_environment'])), {\"query\": \"from:{} lang:en\".format(str(datafile['target_user_id'])), \"toDate\": str(datafile['toDate']), \"fromDate\": str(datafile['fromDate']), \"maxResults\": int(datafile['max_results']), \"next\": str(next_token)})\n",
    "\n",
    "    #Time to write out some files now that the tweet pull down is done!\n",
    "    #it would be easy enough to write out a json file and a csv file for the tweet list and brand_tweets list\n",
    "    with(open(str(datafile['output_file_name']) + '.json', 'w')) as json_tweets:\n",
    "        json.dump(TWEET_LIST, json_tweets)\n",
    "    tweet_list_df = pandas.DataFrame(TWEET_LIST)\n",
    "    tweet_list_df.to_csv(str(datafile['output_file_name']) + '.csv', encoding='utf-8')\n",
    "\n",
    "    with(open('brand_' + str(datafile['output_file_name']) + '.json', 'w')) as json_brand:\n",
    "        json.dump(BRAND_LIST, json_brand)\n",
    "    brand_list_df = pandas.DataFrame(BRAND_LIST)\n",
    "    brand_list_df.to_csv('brand_' + str(datafile['output_file_name']) + '.csv', encoding='utf-8')\n",
    "\n",
    "    #also need to close the save file so it doesnt take up any memory while we are working on the webscraper\n",
    "    savefile.close()\n",
    "\n",
    "    #after running this a few times and getting a div by zero error, going to throw a try/catch block here\n",
    "    try:\n",
    "        print(\"Total number of tweets pulled: {}. Of those, {}% were from the brand ({} tweets)\".format(len(TWEET_LIST), (100 * len(BRAND_LIST)/len(TWEET_LIST)), len(BRAND_LIST)))\n",
    "    except ZeroDivisionError as err:\n",
    "        print(\"Okay so we have one of two cases: There are no brand tweets to scrape comments from, or we have run out of requests and were unable to find any tweets (see above for an error code 429).\")\n",
    "\n",
    "    #I am going to start the webscraping portion of the program here.\n",
    "    print(\"Starting webscraper in\", end=\" \")\n",
    "    for i in range(0,3):\n",
    "        print(3 - i)\n",
    "        #lmao this might trip someone up, probably not tho and this is just for entertainment\n",
    "        time.sleep(.9)\n",
    "\n",
    "    #i wrote a helper function to do all of the webscraping, which you can see above in the HELPERS section, but this will pull all of the comments for the brand tweets\n",
    "    comment_tweets = scrape_comments(datafile, BRAND_LIST)\n",
    "    #now i can just dump them out to the local directory the same way as before, in json and in csv\n",
    "    with open(\"{}.json\".format(str(datafile[\"output_file_name\"])), 'w') as comments_json:\n",
    "        json.dump(comment_tweets, comments_json)\n",
    "    comment_tweets_df = pandas.DataFrame(comment_tweets)\n",
    "    comment_tweets_df.to_csv(\"{}.csv\".format(str(datafile[\"output_file_name\"])), encoding=\"utf-8\")\n",
    "\n",
    "    #adding a little helper function call down here for a verbose output file, it will combine the brand tweets and the comment tweets into one table or json file. this will be used for the Power BI export. making it as flat as possible\n",
    "    if(verbose.lower() == 'yes'):\n",
    "        print(\"Beginning the verbose data dump... creating merged table of brand tweets and comment tweets\")\n",
    "        merged_brand_comment_df = merge_brand_and_comments_on_id(datafile, brand_list_df, comment_tweets_df)\n",
    "        make_raspi_datafile(datafile['output_file_name'], merged_brand_comment_df)\n",
    "\n",
    "    #okay so change of plans, instead of building the streamlit app in here, I might build it in a sep file and then call it here, or chain these two scipts together\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
